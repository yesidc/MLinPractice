{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121fb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import ast\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e442e0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8af0036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good Morning How are you'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "regular_punct = list(string.punctuation)\n",
    "def remove_punctuation(text,punct_list):\n",
    "    for punc in punct_list:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, ' ')\n",
    "    return text.strip()\n",
    "\n",
    "remove_punctuation(\" Good Morning!How are you? \",regular_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c70f0e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/preprocessing/preprocessed.csv\",nrows=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09ed2443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['â€™', 'video', 'made', 'abt', 'last', 'night', 'nb', 'broadcaster', 'try', 'steer', 'clear', 'deep', 'data', 'analysis', 'chart', 'like', 'think', 'viewer', 'switch', 'skynews', 'care', 'stuff', 'think', 'viewer']\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_tweet\"][19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97b86c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hereâ€™s a video we made abt this last night NB most other broadcasters try to steer clear of deep data analysis and charts like these They think viewers will switch off Not SkyNews who care about this stuff and think their viewers do too  httpstcos1GMIEGK11'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweet_no_punctuation\"][19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b6f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d859ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DA officials claim their analysis says thereâ€™s pork shortage and allowed importation of 200 million kilos every year from 2018 2020  The PSA data indicate there was local production surplus of 400 million kilos per year  Iâ€™ll side with data  Analysis can be self serving'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation(df[\"tweet\"][0],regular_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "980b593a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'conversation_id', 'created_at', 'date', 'time', 'timezone',\n",
       "       'user_id', 'username', 'name', 'place', 'tweet', 'language', 'mentions',\n",
       "       'urls', 'photos', 'replies_count', 'retweets_count', 'likes_count',\n",
       "       'hashtags', 'cashtags', 'link', 'retweet', 'quote_url', 'video',\n",
       "       'thumbnail', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
       "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
       "       'trans_dest\\r', 'label', 'tweet_no_punctuation',\n",
       "       'tweet_no_punctuation_tokenized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22df359c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Dr', 'Tony', 'Liu', 'Boragen', 'Better', 'data', 'analysis', 'help', 'u', 'uncover', 'innovation', 'escape', 'human', 'eye', 'SAS', 'essential', 'tool', 'helping', 'u', 'discover', 'efficient', 'sustainable', 'solution', 'pressing', 'agricultural', 'challenge', 'agtech', 'httpstco0mpICma2AT']\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet_no_punctuation_tokenized'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8448552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr Tony Liu Boragen Better data analysis can help us uncover innovations that escape the human eye SAS will be an essential tool for helping us discover more efficient and sustainable solutions to the most pressing agricultural challenges agtech  httpstco0mpICma2AT'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweet_no_punctuation\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e508788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      DA officials claim their analysis says thereâ€™s...\n",
       "1      If you need clarification this should help. I ...\n",
       "2      Geospatial Technology Important to Large-Scale...\n",
       "3      Dr. Tony Liu, Boragen: \"Better data analysis c...\n",
       "4      Palantir is continuing its push into life scie...\n",
       "                             ...                        \n",
       "195    #Upskill with our Spreadsheet Software Level 2...\n",
       "196    ðŸ“¢ Last week's webinar is available to stream ðŸ“½...\n",
       "197    Did you know that #dataquality has a huge impa...\n",
       "198    The results here are geared towards fantasy pl...\n",
       "199    Xero's latest data analysis for Feb (5th conse...\n",
       "Name: tweet, Length: 200, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3e889884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized = []\n",
    "filtered_sentence = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "for tweeter in df['tweet_no_punctuation']: \n",
    "    tweet = remove_url(tweeter)\n",
    "            \n",
    "    # setp 1: convert the tweets into sentences\n",
    "            \n",
    "    sentences = nltk.sent_tokenize(tweet) # (tweet, language = \"English\") , \n",
    "                                                    #can it solve the problem of selecting the only english tweets?? have to \n",
    "    tokenized_tweet = []\n",
    "    lemma_word_list = []\n",
    "            #step 2: convert sentences into word tokens\n",
    "            \n",
    "    for sentence in sentences: \n",
    "        words= nltk.word_tokenize(sentence)\n",
    "                \n",
    "                #Step 3: Remove stop words from the word tokens \n",
    "        #stop_removed = [word for word in words if not word in stop_words]\n",
    "        stop_removed = []\n",
    "        for word in words:\n",
    "            if not word in stop_words:\n",
    "                \n",
    "                lem_word = lemmatizer.lemmatize(word)\n",
    "                stop_removed.append(lem_word)\n",
    "                \n",
    "\n",
    "        tokenized_tweet += stop_removed\n",
    "            \n",
    "    tokenized.append(str(tokenized_tweet))\n",
    "            \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "732e77a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'conversation_id', 'created_at', 'date', 'time', 'timezone',\n",
       "       'user_id', 'username', 'name', 'place', 'tweet', 'language', 'mentions',\n",
       "       'urls', 'photos', 'replies_count', 'retweets_count', 'likes_count',\n",
       "       'hashtags', 'cashtags', 'link', 'retweet', 'quote_url', 'video',\n",
       "       'thumbnail', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
       "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
       "       'trans_dest\\r', 'label', 'tweet_no_punctuation', 'tweet_tokenized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d6b87cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today, join @SKtelecom and NVIDIA to learn how to optimize network data analysis using accelerated #deeplearning with Apache Spark and RAPIDS.  Attend session S31400 at #GTC21:  https://t.co/uyZgEG1nvB  https://t.co/B0eFswmMdM'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d212c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Geospatial Technology Important to LargeScale Data Analysis  '"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = df['tweet_no_punctuation'][2]\n",
    "\n",
    "remove_url(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "16a60a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['DA', 'official', 'claim', 'analysis', 'say', 'â€™', 'pork', 'shortage', 'allowed', 'importation', '200', 'million', 'kilo', 'every', 'year', '20182020', 'The', 'PSA', 'data', 'indicate', 'local', 'production', 'surplus', '400', 'million', 'kilo', 'per', 'year', 'I', 'â€™', 'side', 'data', 'Analysis', 'selfserving']\",\n",
       " \"['If', 'need', 'clarification', 'help', 'I', 'blend', 'psychology', 'data', 'analysis', 'date', 'news', 'partnership', 'investment', 'together', 'bring', 'detailed', 'information', 'cryptocurrency', 'Today', 'I', 'â€™', 'dropping', 'dogecoin', '3pm', 'pst', '365', 'Million', 'reason']\",\n",
       " \"['Geospatial', 'Technology', 'Important', 'LargeScale', 'Data', 'Analysis']\",\n",
       " \"['Dr', 'Tony', 'Liu', 'Boragen', 'Better', 'data', 'analysis', 'help', 'u', 'uncover', 'innovation', 'escape', 'human', 'eye', 'SAS', 'essential', 'tool', 'helping', 'u', 'discover', 'efficient', 'sustainable', 'solution', 'pressing', 'agricultural', 'challenge', 'agtech']\",\n",
       " \"['Palantir', 'continuing', 'push', 'life', 'science', 'customer', 'like', 'Merck', 'Sanofi', 'secretive', 'data', 'analysis', 'firm', 'expands', 'beyond', 'defense', 'deal']\"]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0:5]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e6e39259",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ast.literal_eval(tokenized[0])\n",
    "cv\n",
    "type(cv)\n",
    "p = tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aadc916d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['DA'\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5240a8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DA', 'official', 'claim', 'analysis', 'say']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67b7fec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr. Tony Liu, Boragen: \"Better data analysis can help us uncover innovations that escape the human eye. SAS will be an essential tool for helping us discover more efficient and sustainable solutions to the most pressing agricultural challenges.\" #agtech  https://t.co/0mpICma2AT'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweet\"][3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fdbf58b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you need clarification this should help. I blend psychology, data, analysis, dates, news, partnership and investments together to bring you detailed information about cryptocurrency. Today Iâ€™m dropping @dogecoin at 3pm pst here -   | 365 Million reasons  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_url(text_data):\n",
    "  \"\"\"remove_url takes raw text and removes urls from the text.\n",
    "     https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python/40823105#40823105\n",
    "     \"\"\"\n",
    "  return re.sub(r\"http\\S+\", \"\", text_data)\n",
    "\n",
    "processed_text = remove_url(c)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e57c313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you need clarification this should help. I blend psychology, data, analysis, dates, news, partnership and investments together to bring you detailed information about cryptocurrency. Today Iâ€™m dropping @dogecoin at 3pm pst here -  https://t.co/7kUNXZJ0Yg | 365 Million reasons  https://t.co/QzzDm6qkAY'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723df54",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
