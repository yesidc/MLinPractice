#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Oct 17 13:01:57 2021

@author: ArghaSarker
"""
# importing the necessary library and files
from script.preprocessing.preprocessor import Preprocessor
from script.util import COLUMN_TWEET

import nltk


class Tokenizer(Preprocessor):
    """Tokenize the tweets (sentences into words )"""
    
    # initializing inti to handle the class 
    
    def __init__(self, input_column, output_column):
        """input column takes the input and stores it 
        from data and output_column store our tokenized 
        data and sends the result out"""
        
        # input column "tweet", new output column
        super().__init__([input_column], output_column)
        
    # now we get values from the preprocessor class
    
    def _get_values(self, inputs):
        # changes the tweets into first sentences and then to words and outputs as token
        # holdes our tokenized words
        tokenized = []
        
        for tweet in inputs[0]: 
            
            # setp 1: convert the tweets into sentences
            
            sentences = nltk.sent_tokenize(tweet) # (tweet, language = "English") , 
                                                    #can it solve the problem of selecting the only english tweets?? have to 
            tokenized_tweet = []
            
            #step 2: convert sentences into word tokens
            
            for sentence in sentences: 
                words= nltk.word_tokenize(sentence)
                tokenized_tweet += words
            
            tokenized.append(str(tokenized_tweet))
            
            
                
                
                
            
        
        
        
    
    
    